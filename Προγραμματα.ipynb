{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Μηχανή Αναζήτησης - Συγκεντρωμένα Προγράμματα\n",
    "\n",
    "Στο Notebook αυτό θα βρεις **όλα τα προγράμματα** που απαρτίζουν τη μηχανή αναζήτησης:\n",
    "\n",
    "1. **Wikipedia Crawler** (Συλλογή δεδομένων)\n",
    "2. **Preprocessing Script** (Προεπεξεργασία κειμένου)\n",
    "3. **create_inverted_index** (Δημιουργία απλού Inverted Index)\n",
    "4. **create_inverted_index_with_tfidf** (Δημιουργία Inverted Index με TF-IDF)\n",
    "5. **Boolean Search Engine** (Χρησιμοποιεί απλό inverted index)\n",
    "6. **Enhanced Search Engine** (TF-IDF, VSM, BM25)\n",
    "7. **Evaluation** (Υπολογισμός μετρικών ακρίβειας, ανάκλησης, F1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Wikipedia Crawler\n",
    "\n",
    "**Σκοπός**: Συλλέγει δεδομένα από το Wikipedia, αντλώντας τον τίτλο και την πρώτη παράγραφο από κάθε σελίδα. Στη συνέχεια, τα αποθηκεύει σε μορφή JSON (π.χ. `wiki_definitions.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def fetch_html(url):\n",
    "    \"\"\"\n",
    "    Στέλνει αίτημα GET σε ένα URL και επιστρέφει το HTML της σελίδας.\n",
    "    Επιστρέφει None αν υπάρξει σφάλμα δικτύου ή άλλο HTTP σφάλμα.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Εάν το response != 200, σηκώνεται exception\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_first_paragraph(html):\n",
    "    \"\"\"\n",
    "    Αναλύει το HTML με BeautifulSoup και επιστρέφει τον τίτλο και την πρώτη\n",
    "    παράγραφο που βρει.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Αφαιρούμε tags <sup> (συνήθως citations)\n",
    "    for sup_tag in soup.find_all('sup'):\n",
    "        sup_tag.decompose()\n",
    "\n",
    "    # Αφαιρούμε tags <span class=\"reference\"> (επίσης citations)\n",
    "    for ref_tag in soup.find_all('span', class_='reference'):\n",
    "        ref_tag.decompose()\n",
    "\n",
    "    # Βρίσκουμε τον τίτλο από το <title>\n",
    "    title_tag = soup.find('title')\n",
    "    title = title_tag.text if title_tag else \"No Title\"\n",
    "\n",
    "    # Παίρνουμε την πρώτη παράγραφο\n",
    "    paragraphs = soup.find_all('p')\n",
    "    first_paragraph = \"\"\n",
    "    for p in paragraphs:\n",
    "        text = p.get_text(separator=\" \", strip=True)\n",
    "        if text:\n",
    "            first_paragraph = text\n",
    "            break\n",
    "\n",
    "    return title, first_paragraph\n",
    "\n",
    "def collect_wikipedia_definitions(url_list, output_filename=\"wiki_definitions.json\"):\n",
    "    \"\"\"\n",
    "    Δέχεται μια λίστα από Wikipedia URLs και για κάθε σελίδα:\n",
    "      - Ανακτά HTML,\n",
    "      - Εξάγει τον τίτλο και την πρώτη παράγραφο,\n",
    "      - Αποθηκεύει σε JSON.\n",
    "    \"\"\"\n",
    "    collected_data = []\n",
    "\n",
    "    for url in url_list:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        html = fetch_html(url)\n",
    "        if html:\n",
    "            title, first_paragraph = extract_first_paragraph(html)\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'definition': first_paragraph\n",
    "            }\n",
    "            collected_data.append(data)\n",
    "        else:\n",
    "            data = {\n",
    "                'url': url,\n",
    "                'title': \"Error\",\n",
    "                'definition': \"Could not fetch page\"\n",
    "            }\n",
    "            collected_data.append(data)\n",
    "\n",
    "    # Τέλος, αποθήκευση σε αρχείο JSON\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(collected_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\nTotal pages fetched: {len(collected_data)}\")\n",
    "    print(f\"Data saved to '{output_filename}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikipedia_pages = [\n",
    "        \"https://en.wikipedia.org/wiki/Web_crawler\",\n",
    "        \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n",
    "        # ... (βάλε εδώ όλα τα URLs που έχεις)\n",
    "    ]\n",
    "\n",
    "    collect_wikipedia_definitions(\n",
    "        url_list=wikipedia_pages,\n",
    "        output_filename=\"wiki_definitions.json\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση Συναρτήσεων (Wikipedia Crawler)\n",
    "\n",
    "- **`fetch_html(url)`**:\n",
    "  - Στέλνει αίτημα GET στο `url`.\n",
    "  - Κάνει `response.raise_for_status()` για να πιάσει σφάλματα HTTP.\n",
    "  - Επιστρέφει το `response.text` αν όλα πάνε καλά.\n",
    "\n",
    "- **`extract_first_paragraph(html)`**:\n",
    "  - Χρησιμοποιεί `BeautifulSoup` για ανάλυση HTML.\n",
    "  - Αφαιρεί citations (`<sup>`, `<span class=\"reference\">`).\n",
    "  - Βρίσκει τον τίτλο με `soup.find('title')`.\n",
    "  - Παίρνει την πρώτη **μη κενή** παράγραφο.\n",
    "\n",
    "- **`collect_wikipedia_definitions(url_list, output_filename)`**:\n",
    "  - Για κάθε URL, καλεί `fetch_html` και `extract_first_paragraph`.\n",
    "  - Δημιουργεί μια λίστα `collected_data` και την αποθηκεύει σε `wiki_definitions.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Προεπεξεργασία Κειμένου (Preprocessing)\n",
    "\n",
    "**Σκοπός**: Παίρνει το `wiki_definitions.json` και καθαρίζει τα κείμενα. Κάνει μετατροπή σε πεζά, αφαίρεση ειδικών χαρακτήρων, stopwords, stemming, και αποθηκεύει την επεξεργασμένη μορφή σε `wiki_definitions_cleaned.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - Μετατρέπει σε lowercase\n",
    "    - Αφαιρεί ειδικούς χαρακτήρες\n",
    "    - Χωρίζει το κείμενο σε tokens\n",
    "    - Αφαιρεί stopwords\n",
    "    - Εφαρμόζει stemming\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Αφαιρούμε ειδικούς χαρακτήρες διατηρώντας μόνο γράμματα/αριθμούς\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "    # 3. Tokenization (απλό split)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # 4. Αφαίρεση stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "def main(input_json, output_json):\n",
    "    \"\"\"\n",
    "    1. Διαβάζει το input_json\n",
    "    2. Καλεί preprocess_text για κάθε 'definition'\n",
    "    3. Αποθηκεύει σε output_json\n",
    "    \"\"\"\n",
    "    with open(input_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        original_definition = item.get('definition', '')\n",
    "        cleaned_tokens = preprocess_text(original_definition)\n",
    "\n",
    "        new_item = {\n",
    "            'url': item.get('url', ''),\n",
    "            'title': item.get('title', ''),\n",
    "            'original_definition': original_definition,\n",
    "            'cleaned_definition': cleaned_tokens\n",
    "        }\n",
    "        cleaned_data.append(new_item)\n",
    "\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Preprocessing complete. Saved to {output_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"wiki_definitions.json\"\n",
    "    output_file = \"wiki_definitions_cleaned.json\"\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση Συναρτήσεων (Προεπεξεργασία)\n",
    "\n",
    "- **`preprocess_text(text)`**:\n",
    "  - Χρησιμοποιεί **regular expressions** για να αφαιρέσει ειδικούς χαρακτήρες.\n",
    "  - Κάνει **tokenization** με `split()`.\n",
    "  - Αφαιρεί **stopwords** μέσω `stopwords.words('english')`.\n",
    "  - Εφαρμόζει **PorterStemmer()** για να πάρει τις ρίζες των λέξεων.\n",
    "\n",
    "- **`main(input_json, output_json)`**:\n",
    "  - Διαβάζει το αρχείο `wiki_definitions.json`.\n",
    "  - Για κάθε εγγραφή, προεπεξεργάζεται το `definition`.\n",
    "  - Αποθηκεύει το αποτέλεσμα στο `wiki_definitions_cleaned.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Δημιουργία Inverted Index\n",
    "\n",
    "**Σκοπός**: Διαβάζει τα \"καθαρισμένα\" κείμενα από το `wiki_definitions_cleaned.json` και φτιάχνει ένα απλό inverted index στη μορφή `λέξη -> [doc_ids]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_inverted_index(input_json, output_json):\n",
    "    \"\"\"\n",
    "    1. Διαβάζει τα καθαρισμένα δεδομένα (wiki_definitions_cleaned.json)\n",
    "    2. Δημιουργεί ανεστραμμένο ευρετήριο (inverted index)\n",
    "    3. Αποθηκεύει σε output_json\n",
    "    \"\"\"\n",
    "    with open(input_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    for i, doc in enumerate(data):\n",
    "        doc_id = i + 1  # ξεκινάμε από 1\n",
    "        tokens = doc.get('cleaned_definition', [])\n",
    "        for token in tokens:\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token].append(doc_id)\n",
    "\n",
    "    # Μετατρέπουμε σε κανονικό dict\n",
    "    inverted_index = dict(inverted_index)\n",
    "\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Inverted index created and saved to {output_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"wiki_definitions_cleaned.json\"\n",
    "    output_file = \"inverted_index.json\"\n",
    "    create_inverted_index(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση (create_inverted_index)\n",
    "\n",
    "- **`create_inverted_index`**:\n",
    "  - Χρησιμοποιεί `defaultdict(list)` για να προσθέτει doc_ids σε λίστες.\n",
    "  - Κάθε token αντιστοιχίζεται σε όλα τα `doc_id` όπου εμφανίζεται.\n",
    "  - Αποθηκεύει ένα dict του τύπου:\n",
    "    ```json\n",
    "    {\n",
    "      \"machine\": [1, 5, 12],\n",
    "      \"learn\": [1, 8]\n",
    "      ...\n",
    "    }\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Inverted Index με TF-IDF\n",
    "\n",
    "**Σκοπός**: Επιπλέον αποθηκεύουμε τη συχνότητα εμφάνισης μιας λέξης σε ένα doc (TF) και την τιμή **TF-IDF**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def create_inverted_index_with_tfidf(input_json, output_json):\n",
    "    with open(input_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total_docs = len(data)\n",
    "\n",
    "    freq_index = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Υπολογίζουμε πόσες φορές εμφανίζεται μια λέξη σε κάθε doc (TF)\n",
    "    for i, doc in enumerate(data):\n",
    "        doc_id = i + 1\n",
    "        tokens = doc.get('cleaned_definition', [])\n",
    "        for token in tokens:\n",
    "            freq_index[token][doc_id] += 1\n",
    "\n",
    "    # Υπολογισμός IDF\n",
    "    idf_scores = {}\n",
    "    for term, docs_dict in freq_index.items():\n",
    "        df = len(docs_dict)  # πόσα docs έχουν αυτό το term\n",
    "        idf_scores[term] = math.log(total_docs / (1 + df))\n",
    "\n",
    "    # Δημιουργία inverted index με tf & tf-idf\n",
    "    inverted_index = {}\n",
    "    for term, docs_dict in freq_index.items():\n",
    "        inverted_index[term] = {}\n",
    "        for doc_id, tf in docs_dict.items():\n",
    "            tf_idf = tf * idf_scores[term]\n",
    "            inverted_index[term][doc_id] = {\n",
    "                \"tf\": tf,\n",
    "                \"tf-idf\": tf_idf\n",
    "            }\n",
    "\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Inverted index with TF and TF-IDF saved to {output_json}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"wiki_definitions_cleaned.json\"\n",
    "    output_file = \"inverted_index_tfidf.json\"\n",
    "    create_inverted_index_with_tfidf(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση (create_inverted_index_with_tfidf)\n",
    "\n",
    "- **TF** (Term Frequency): Για κάθε `(term, doc_id)` μετράμε πόσες φορές εμφανίζεται η λέξη στο doc.\n",
    "- **IDF** (Inverse Document Frequency): `idf = log(total_docs / (1 + df))`, όπου `df` είναι πόσα docs περιέχουν τη λέξη.\n",
    "- **tf-idf** = TF * IDF.\n",
    "- Το τελικό inverted index αποθηκεύεται σε μορφή:\n",
    "  ```json\n",
    "  {\n",
    "    \"machine\": {\n",
    "      1: {\"tf\": 3, \"tf-idf\": 3.5},\n",
    "      5: {\"tf\": 1, \"tf-idf\": 1.2},\n",
    "      ...\n",
    "    },\n",
    "    ...\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Απλή Μηχανή Αναζήτησης (Boolean)\n",
    "\n",
    "**Σκοπός**: Χρησιμοποιεί το `inverted_index.json` (χωρίς TF-IDF). Δίνει τη δυνατότητα Boolean αναζητήσεων (AND, OR, NOT) ή απλών (default OR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def load_inverted_index(index_file):\n",
    "    try:\n",
    "        with open(index_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {index_file}.\")\n",
    "        return None\n",
    "\n",
    "def load_documents(docs_file):\n",
    "    \"\"\"\n",
    "    Φορτώνει τη λίστα των εγγράφων (JSON)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(docs_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {docs_file}.\")\n",
    "        return None\n",
    "\n",
    "def preprocess_query(query):\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^a-z0-9\\s]', ' ', query)\n",
    "    tokens = query.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def process_boolean_query(tokens, inverted_index):\n",
    "    result_set = None\n",
    "    operation = None\n",
    "    for token in tokens:\n",
    "        if token in [\"and\", \"or\", \"not\"]:\n",
    "            operation = token\n",
    "        else:\n",
    "            docs_for_token = set(map(int, inverted_index.get(token, [])))\n",
    "            if result_set is None:\n",
    "                result_set = docs_for_token\n",
    "            else:\n",
    "                if operation == \"and\":\n",
    "                    result_set &= docs_for_token\n",
    "                elif operation == \"or\":\n",
    "                    result_set |= docs_for_token\n",
    "                elif operation == \"not\":\n",
    "                    result_set -= docs_for_token\n",
    "                else:\n",
    "                    result_set |= docs_for_token\n",
    "    if result_set is None:\n",
    "        result_set = set()\n",
    "    return result_set\n",
    "\n",
    "def process_simple_query(tokens, inverted_index, default_operator=\"or\"):\n",
    "    result_set = set()\n",
    "    for i, token in enumerate(tokens):\n",
    "        docs_for_token = set(map(int, inverted_index.get(token, [])))\n",
    "        if i == 0:\n",
    "            result_set = docs_for_token\n",
    "        else:\n",
    "            if default_operator == \"or\":\n",
    "                result_set |= docs_for_token\n",
    "            elif default_operator == \"and\":\n",
    "                result_set &= docs_for_token\n",
    "    return result_set\n",
    "\n",
    "def process_query(query, inverted_index, default_operator=\"or\"):\n",
    "    tokens = query.lower().split()\n",
    "    has_boolean_ops = any(tok in [\"and\", \"or\", \"not\"] for tok in tokens)\n",
    "    if has_boolean_ops:\n",
    "        stemmed_tokens = preprocess_query(query)\n",
    "        return process_boolean_query(stemmed_tokens, inverted_index)\n",
    "    else:\n",
    "        stemmed_tokens = preprocess_query(query)\n",
    "        return process_simple_query(stemmed_tokens, inverted_index, default_operator)\n",
    "\n",
    "def main():\n",
    "    index_file = \"inverted_index.json\"\n",
    "    docs_file = \"wiki_definitions_cleaned.json\"\n",
    "\n",
    "    inverted_index = load_inverted_index(index_file)\n",
    "    if not inverted_index:\n",
    "        return\n",
    "    documents = load_documents(docs_file)\n",
    "    if not documents:\n",
    "        return\n",
    "\n",
    "    print(\"Welcome to the Search Engine!\")\n",
    "    print(\"(Boolean operators: AND, OR, NOT) or just words.\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        result_docs = process_query(query, inverted_index)\n",
    "        if result_docs:\n",
    "            print(f\"\\nFound {len(result_docs)} documents:\")\n",
    "            for doc_id in sorted(result_docs):\n",
    "                real_index = doc_id - 1\n",
    "                if 0 <= real_index < len(documents):\n",
    "                    doc = documents[real_index]\n",
    "                    title = doc.get('title', 'No Title')\n",
    "                    original_def = doc.get('original_definition', 'No Definition')\n",
    "                    snippet = original_def[:200] + \"...\" if len(original_def) > 200 else original_def\n",
    "                    print(f\"  Document ID: {doc_id}\")\n",
    "                    print(f\"    Title: {title}\")\n",
    "                    print(f\"    Definition: {snippet}\\n\")\n",
    "                else:\n",
    "                    print(f\"  Document ID: {doc_id} (not found)\\n\")\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση (Boolean Search)\n",
    "\n",
    "- **`process_boolean_query(tokens, inverted_index)`**:\n",
    "  - Διαβάζει διαδοχικά τους tokens.\n",
    "  - Αν συναντήσει λογικό τελεστή (`AND`, `OR`, `NOT`), ενημερώνει την τρέχουσα πράξη.\n",
    "  - Αν συναντήσει λέξη, βρίσκει τα doc_ids από το `inverted_index`.\n",
    "  - Εφαρμόζει την τρέχουσα πράξη στο `result_set`.\n",
    "\n",
    "- **`process_simple_query(tokens, ...)`**:\n",
    "  - Υλοποιεί OR/AND σε όλα τα tokens αν δεν υπάρχει ρητός τελεστής.\n",
    "\n",
    "- **`process_query(query, inverted_index, default_operator)`**:\n",
    "  - Προεπεξεργάζεται το query με stemming κ.λπ.\n",
    "  - Επιλέγει αν θα εκτελέσει Boolean ή απλή αναζήτηση.\n",
    "\n",
    "- **Εκτέλεση**:\n",
    "  - Φορτώνει `inverted_index.json`, `wiki_definitions_cleaned.json`.\n",
    "  - Δέχεται ερώτημα χρήστη.\n",
    "  - Εμφανίζει doc_ids και αποσπάσματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Εμπλουτισμένη Μηχανή Αναζήτησης (TF-IDF, VSM, BM25)\n",
    "\n",
    "**Σκοπός**: Φορτώνει το `inverted_index_tfidf.json`, δίνει δυνατότητα **αξιοποίησης TF-IDF**, **Vector Space Model** (cosine similarity), και **Okapi BM25**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def load_inverted_index(index_file):\n",
    "    try:\n",
    "        with open(index_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {index_file}.\")\n",
    "        return None\n",
    "\n",
    "def load_documents(docs_file):\n",
    "    try:\n",
    "        with open(docs_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find the file {docs_file}.\")\n",
    "        return None\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def preprocess_documents(documents):\n",
    "    processed_texts = []\n",
    "    for doc in documents:\n",
    "        original_def = doc.get('original_definition', '')\n",
    "        tokens = preprocess_text(original_def)\n",
    "        processed_text = ' '.join(tokens)\n",
    "        processed_texts.append(processed_text)\n",
    "    return processed_texts\n",
    "\n",
    "def initialize_vsm(processed_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "def initialize_bm25(processed_texts):\n",
    "    tokenized_texts = [text.split() for text in processed_texts]\n",
    "    bm25 = BM25Okapi(tokenized_texts)\n",
    "    return bm25\n",
    "\n",
    "# boolean ή απλή αναζήτηση\n",
    "def process_boolean_query(tokens, inverted_index):\n",
    "    result_set = None\n",
    "    operation = None\n",
    "    for token in tokens:\n",
    "        if token in [\"and\", \"or\", \"not\"]:\n",
    "            operation = token\n",
    "        else:\n",
    "            docs_for_token = {int(doc) for doc in inverted_index.get(token, [])}\n",
    "            if result_set is None:\n",
    "                result_set = docs_for_token\n",
    "            else:\n",
    "                if operation == \"and\":\n",
    "                    result_set &= docs_for_token\n",
    "                elif operation == \"or\":\n",
    "                    result_set |= docs_for_token\n",
    "                elif operation == \"not\":\n",
    "                    result_set -= docs_for_token\n",
    "                else:\n",
    "                    result_set |= docs_for_token\n",
    "    return result_set if result_set is not None else set()\n",
    "\n",
    "def process_simple_query(tokens, inverted_index, default_operator=\"or\"):\n",
    "    result_set = set()\n",
    "    for i, token in enumerate(tokens):\n",
    "        docs_for_token = {int(doc) for doc in inverted_index.get(token, [])}\n",
    "        if i == 0:\n",
    "            result_set = docs_for_token\n",
    "        else:\n",
    "            if default_operator == \"or\":\n",
    "                result_set |= docs_for_token\n",
    "            elif default_operator == \"and\":\n",
    "                result_set &= docs_for_token\n",
    "    return result_set\n",
    "\n",
    "def calculate_ranking_boolean(result_docs, inverted_index, query_terms, vectorizer, tfidf_matrix):\n",
    "    doc_scores = {}\n",
    "    for term in query_terms:\n",
    "        if term in vectorizer.vocabulary_:\n",
    "            term_index = vectorizer.vocabulary_[term]\n",
    "            term_tfidf = tfidf_matrix[:, term_index].toarray().flatten()\n",
    "            for doc_id in result_docs:\n",
    "                doc_id = int(doc_id)\n",
    "                score = term_tfidf[doc_id - 1]\n",
    "                doc_scores[doc_id] = doc_scores.get(doc_id, 0) + score\n",
    "    return doc_scores\n",
    "\n",
    "def calculate_ranking_vsm(query, vectorizer, tfidf_matrix):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    return cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "def calculate_ranking_bm25(query, bm25):\n",
    "    tokens = query.split()\n",
    "    return bm25.get_scores(tokens)\n",
    "\n",
    "def process_query(\n",
    "    query, inverted_index, vectorizer, tfidf_matrix, bm25, documents,\n",
    "    default_operator=\"or\"\n",
    "):\n",
    "    tokens = query.lower().split()\n",
    "    has_boolean_ops = any(tok in [\"and\", \"or\", \"not\"] for tok in tokens)\n",
    "\n",
    "    if has_boolean_ops:\n",
    "        stemmed_tokens = preprocess_text(query)\n",
    "        result_docs = process_boolean_query(stemmed_tokens, inverted_index)\n",
    "        if result_docs:\n",
    "            query_terms = [tok for tok in stemmed_tokens if tok not in [\"and\", \"or\", \"not\"]]\n",
    "            doc_scores = calculate_ranking_boolean(\n",
    "                result_docs, inverted_index, query_terms, vectorizer, tfidf_matrix\n",
    "            )\n",
    "            ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            ranked_docs = []\n",
    "    else:\n",
    "        stemmed_tokens = preprocess_text(query)\n",
    "        result_docs = process_simple_query(stemmed_tokens, inverted_index, default_operator)\n",
    "        if result_docs:\n",
    "            print(\"\\nSelect Ranking Algorithm:\")\n",
    "            print(\"1. TF-IDF Sum\")\n",
    "            print(\"2. Vector Space Model (Cosine Similarity)\")\n",
    "            print(\"3. Okapi BM25\")\n",
    "            choice = input(\"Enter the number of the ranking algorithm (1-3): \")\n",
    "\n",
    "            if choice == \"1\":\n",
    "                query_terms = [tok for tok in stemmed_tokens]\n",
    "                doc_scores = calculate_ranking_boolean(\n",
    "                    result_docs, inverted_index, query_terms,\n",
    "                    vectorizer, tfidf_matrix\n",
    "                )\n",
    "                ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            elif choice == \"2\":\n",
    "                original_query = ' '.join(stemmed_tokens)\n",
    "                cosine_similarities = calculate_ranking_vsm(original_query, vectorizer, tfidf_matrix)\n",
    "                ranked_docs = sorted([\n",
    "                    (doc_id, cosine_similarities[doc_id - 1])\n",
    "                    for doc_id in result_docs\n",
    "                ], key=lambda x: x[1], reverse=True)\n",
    "            elif choice == \"3\":\n",
    "                original_query = ' '.join(stemmed_tokens)\n",
    "                bm25_scores = calculate_ranking_bm25(original_query, bm25)\n",
    "                ranked_docs = sorted([\n",
    "                    (doc_id, bm25_scores[doc_id - 1]) for doc_id in result_docs\n",
    "                ], key=lambda x: x[1], reverse=True)\n",
    "            else:\n",
    "                print(\"Invalid choice. Defaulting to TF-IDF Sum.\")\n",
    "                query_terms = [tok for tok in stemmed_tokens]\n",
    "                doc_scores = calculate_ranking_boolean(\n",
    "                    result_docs, inverted_index, query_terms,\n",
    "                    vectorizer, tfidf_matrix\n",
    "                )\n",
    "                ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        else:\n",
    "            ranked_docs = []\n",
    "\n",
    "    return ranked_docs\n",
    "\n",
    "def main():\n",
    "    index_file = \"inverted_index_tfidf.json\"\n",
    "    docs_file = \"wiki_definitions_cleaned.json\"\n",
    "\n",
    "    inverted_index = load_inverted_index(index_file)\n",
    "    if not inverted_index:\n",
    "        return\n",
    "    documents = load_documents(docs_file)\n",
    "    if not documents:\n",
    "        return\n",
    "\n",
    "    print(\"Preprocessing documents for VSM and BM25...\")\n",
    "    processed_texts = preprocess_documents(documents)\n",
    "\n",
    "    print(\"Initializing Vector Space Model (VSM)...\")\n",
    "    vectorizer, tfidf_matrix = initialize_vsm(processed_texts)\n",
    "\n",
    "    print(\"Initializing Okapi BM25...\")\n",
    "    bm25 = initialize_bm25(processed_texts)\n",
    "\n",
    "    print(\"\\nWelcome to the Enhanced Search Engine!\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "        ranked_docs = process_query(\n",
    "            query=query,\n",
    "            inverted_index=inverted_index,\n",
    "            vectorizer=vectorizer,\n",
    "            tfidf_matrix=tfidf_matrix,\n",
    "            bm25=bm25,\n",
    "            documents=documents,\n",
    "            default_operator=\"or\"\n",
    "        )\n",
    "\n",
    "        if ranked_docs:\n",
    "            print(f\"\\nFound {len(ranked_docs)} documents:\")\n",
    "            for doc_id, score in ranked_docs:\n",
    "                real_index = doc_id - 1\n",
    "                if 0 <= real_index < len(documents):\n",
    "                    doc = documents[real_index]\n",
    "                    title = doc.get('title', 'No Title')\n",
    "                    original_def = doc.get('original_definition', 'No Definition')\n",
    "                    snippet = original_def[:200] + \"...\" if len(original_def) > 200 else original_def\n",
    "                    print(f\"  Document ID: {doc_id}\")\n",
    "                    print(f\"    Title: {title}\")\n",
    "                    print(f\"    Definition: {snippet}\")\n",
    "                    print(f\"    Score: {score:.4f}\\n\")\n",
    "        else:\n",
    "            print(\"No results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση (Enhanced Search Engine)\n",
    "\n",
    "- **`initialize_vsm(processed_texts)`**:\n",
    "  - Χρησιμοποιεί `TfidfVectorizer` για να φτιάξει ένα **tfidf_matrix**.\n",
    "\n",
    "- **`initialize_bm25(processed_texts)`**:\n",
    "  - Χρησιμοποιεί `BM25Okapi` από τη βιβλιοθήκη `rank_bm25`.\n",
    "\n",
    "- **`process_query(...)`**:\n",
    "  - Ανιχνεύει αν το query περιέχει τους Boolean operators.\n",
    "  - Αν είναι simple query, ρωτάει τον χρήστη ποιον αλγόριθμο κατάταξης θέλει:\n",
    "    1. **TF-IDF Sum** (απλώς αθροίζει τα tf-idf scores)\n",
    "    2. **Vector Space Model** (cosine similarity)\n",
    "    3. **Okapi BM25**.\n",
    "  - Επιστρέφει ταξινομημένη λίστα `(doc_id, score)`.\n",
    "\n",
    "- **Στο τέλος** εμφανίζει τα έγγραφα και τη βαθμολογία (score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Evaluation Script\n",
    "\n",
    "**Σκοπός**: Εδώ ορίζουμε κάποια *test queries* και τους αντίστοιχους *relevant_docs*, τρέχουμε τη μηχανή αναζήτησης, και υπολογίζουμε μετρικές (precision, recall, F1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Παράδειγμα evaluation\n",
    "from search_engineV2 import (\n",
    "    load_inverted_index,\n",
    "    load_documents,\n",
    "    preprocess_documents,\n",
    "    initialize_vsm,\n",
    "    initialize_bm25,\n",
    "    process_query\n",
    ")\n",
    "\n",
    "def evaluate_system(test_queries, inverted_index, vectorizer, tfidf_matrix, bm25, documents):\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    num_queries = len(test_queries)\n",
    "\n",
    "    for test_query in test_queries:\n",
    "        query = test_query[\"query\"]\n",
    "        relevant_docs = set(test_query[\"relevant_docs\"])\n",
    "\n",
    "        # process_query επιστρέφει ranked_docs = [(doc_id, score), ...]\n",
    "        ranked_docs = process_query(\n",
    "            query=query,\n",
    "            inverted_index=inverted_index,\n",
    "            vectorizer=vectorizer,\n",
    "            tfidf_matrix=tfidf_matrix,\n",
    "            bm25=bm25,\n",
    "            documents=documents,\n",
    "            default_operator=\"or\"\n",
    "        )\n",
    "        retrieved_docs = [doc_id for (doc_id, _) in ranked_docs]\n",
    "\n",
    "        # Υπολογισμός precision, recall, f1\n",
    "        retrieved_set = set(retrieved_docs)\n",
    "        true_positives = len(retrieved_set & relevant_docs)\n",
    "\n",
    "        precision = true_positives / len(retrieved_docs) if retrieved_docs else 0\n",
    "        recall = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"  Relevant Docs: {relevant_docs}\")\n",
    "        print(f\"  Retrieved Docs (top 5): {retrieved_docs[:5]}\")\n",
    "        print(f\"  Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\\n\")\n",
    "\n",
    "    avg_precision = total_precision / num_queries\n",
    "    avg_recall = total_recall / num_queries\n",
    "    avg_f1 = total_f1 / num_queries\n",
    "    print(\"=== AVERAGE RESULTS ===\")\n",
    "    print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "    print(f\"Average Recall:    {avg_recall:.2f}\")\n",
    "    print(f\"Average F1:        {avg_f1:.2f}\")\n",
    "\n",
    "def main():\n",
    "    index_file = \"inverted_index_tfidf.json\"  # Χρησιμοποιούμε το TF-IDF ευρετήριο\n",
    "    docs_file = \"wiki_definitions_cleaned.json\"\n",
    "\n",
    "    inverted_index = load_inverted_index(index_file)\n",
    "    documents = load_documents(docs_file)\n",
    "    if not inverted_index or not documents:\n",
    "        return\n",
    "\n",
    "    processed_texts = preprocess_documents(documents)\n",
    "    vectorizer, tfidf_matrix = initialize_vsm(processed_texts)\n",
    "    bm25 = initialize_bm25(processed_texts)\n",
    "\n",
    "    # Παράδειγμα test queries\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"query\": \"machine learning\",\n",
    "            \"relevant_docs\": [3, 4, 10, 17, 18, 19]  # Π.χ.\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"neural network\",\n",
    "            \"relevant_docs\": [7, 8, 9, 25]  # Παράδειγμα\n",
    "        }\n",
    "        # μπορείς να προσθέσεις κι άλλα\n",
    "    ]\n",
    "\n",
    "    evaluate_system(test_queries, inverted_index, vectorizer, tfidf_matrix, bm25, documents)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Επεξήγηση (Evaluation)\n",
    "\n",
    "- **`evaluate_system(test_queries, inverted_index, ...)`**:\n",
    "  - Για κάθε ερώτημα, καλεί `process_query(...)` ώστε να πάρει τα `(doc_id, score)`.\n",
    "  - Συγκρίνει τα αποτελέσματα (retrieved_docs) με τα `relevant_docs`.\n",
    "  - Υπολογίζει μετρικές:\n",
    "    - **Precision** = `TP / retrieved_docs`\n",
    "    - **Recall** = `TP / total_relevant`\n",
    "    - **F1** = `2 * (precision * recall) / (precision + recall)`\n",
    "  - Εμφανίζει τα **μέσα** αποτελέσματα (Average Precision, Recall, F1).\n",
    "\n",
    "- **`main()`**:\n",
    "  - Φορτώνει το `inverted_index_tfidf.json` και τα έγγραφα.\n",
    "  - Ορίζει κάποια test queries με τα doc_ids που θεωρούνται σχετικά.\n",
    "  - Καλεί τη συνάρτηση evaluate_system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
